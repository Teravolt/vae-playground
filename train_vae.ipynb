{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "import h5py\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import torch\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import wandb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate.utils import GradientAccumulationPlugin\n",
    "from accelerate.utils import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration already exists at /Users/pavankantharaju/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from accelerate.utils import write_basic_config\n",
    "\n",
    "write_basic_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE = torch.device(\n",
    "#     'cuda' if torch.cuda.is_available() \\\n",
    "#         else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "HIDDEN_DIMS = 64\n",
    "\n",
    "CONFIG = Namespace(\n",
    "    project_name=\"3dshapes\",\n",
    "    run_name='3dshapes-run-1',\n",
    "    model_name=f'3dshapes-{HIDDEN_DIMS}-model-v1',\n",
    "    hidden_dims=HIDDEN_DIMS,\n",
    "    horizontal_flip_prob=0.5,\n",
    "    gaussian_blur_kernel_size=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=15,\n",
    "    learning_rate=4e-4,\n",
    "    seed=1,\n",
    "    beta_schedule='squaredcos_cap_v2',\n",
    "    lr_exp_schedule_gamma=0.85,\n",
    "    lr_warmup_steps=500,\n",
    "    train_limit=-1,\n",
    "    save_model=True,\n",
    "    mixed_precision=None,\n",
    "    grad_accumulation_steps=4\n",
    "    )\n",
    "CONFIG.device = DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, h5_data: h5py.File) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.h5_data = h5_data\n",
    "        self.image_shape = self.h5_data['images'][0].shape\n",
    "        self.num_labels = self.h5_data['labels'][0].shape[0]\n",
    "        self.normalize_transform = transforms.Normalize([0.5], [0.5]) # Map to (-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.h5_data['images'])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_array = self.h5_data['images'][index]\n",
    "        labels = self.h5_data['labels'][index]\n",
    "\n",
    "        img_tensor = torch.tensor(img_array, dtype=torch.float32)\n",
    "\n",
    "        img_tensor = img_tensor.transpose(0, 2)\n",
    "        img_tensor = img_tensor/255\n",
    "        # print(f\"Image tensor before: {img_tensor}\")\n",
    "        img_tensor = self.normalize_transform(img_tensor)\n",
    "        # print(f\"Image tensor after: {img_tensor}\")\n",
    "        # raise\n",
    "        labels_tensor = torch.tensor(labels)\n",
    "\n",
    "        # print(f\"Image tensor shape: {img_tensor.shape}\")\n",
    "        # print(f\"Labels tensor shape: {labels_tensor.shape}\")\n",
    "\n",
    "        output = {\n",
    "            'image': img_tensor,\n",
    "            'labels': labels_tensor\n",
    "            }\n",
    "\n",
    "        return output\n",
    "    \n",
    "def create_dataset():\n",
    "    \"\"\"\n",
    "    Create dataset\n",
    "    \"\"\"\n",
    "\n",
    "    data = h5py.File('3dshapes.h5', 'r')\n",
    "    shape_dataset = ShapeDataset(data)\n",
    "    return shape_dataset\n",
    "\n",
    "def prepare_dataloader(config: Namespace):\n",
    "    \"\"\"\n",
    "    Prepare dataloader\n",
    "    \"\"\"\n",
    "\n",
    "    shape_dataset = create_dataset()\n",
    "\n",
    "    generator = torch.Generator().manual_seed(config.seed)\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(shape_dataset, [0.8, 0.2], generator)\n",
    "\n",
    "    train_gen = torch.Generator().manual_seed(config.seed)\n",
    "    val_gen = torch.Generator().manual_seed(config.seed)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=config.per_device_train_batch_size,\n",
    "        shuffle=True, generator=train_gen)\n",
    "\n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=config.per_device_eval_batch_size,\n",
    "        shuffle=True, generator=val_gen)\n",
    "\n",
    "    return train_dataloader, val_dataloader, shape_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ShapeModelEncoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int, dims: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_1 = torch.nn.Conv2d(\n",
    "            in_channels, dims, kernel_size=3, padding='same')\n",
    "        self.max_pool_1 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv_2 = torch.nn.Conv2d(\n",
    "            dims, 2*dims, kernel_size=3, padding='same')\n",
    "        self.max_pool_2 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv_3 = torch.nn.Conv2d(\n",
    "            2*dims, 2*dims, kernel_size=3, padding='same')\n",
    "        self.max_pool_3 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \"\"\"\n",
    "\n",
    "        x_ = self.conv_1(x)\n",
    "        # print(f\"Output of conv 1: {x_.shape}\")\n",
    "        x_ = self.max_pool_1(x_)\n",
    "        # print(f\"Output of conv & max pool 1: {x_.shape}\")\n",
    "\n",
    "        x_ = self.conv_2(x_)\n",
    "        # print(f\"Output of conv 2: {x_.shape}\")\n",
    "        x_ = self.max_pool_2(x_)\n",
    "        # print(f\"Output of conv & max pool 2: {x_.shape}\")\n",
    "\n",
    "        x_ = self.conv_3(x_)\n",
    "        # print(f\"Output of conv 3: {x_.shape}\")\n",
    "        x_ = self.max_pool_3(x_)\n",
    "        # print(f\"Output of conv & max pool 3: {x_.shape}\")\n",
    "\n",
    "        return x_\n",
    "\n",
    "class ShapeModelDecoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_factors: int, dims: int, in_channels: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # NOTE: I still don't understand how transpose convolution works\n",
    "        # self.upsample_3 = torch.nn.Upsample(scale_factor=2)\n",
    "        self.deconv_1 = torch.nn.ConvTranspose2d(num_factors, 2*dims, 2, stride=2)\n",
    "\n",
    "        # self.upsample_2 = torch.nn.Upsample(scale_factor=2)\n",
    "        self.deconv_2 = torch.nn.ConvTranspose2d(2*dims, dims, 2, stride=2)\n",
    "\n",
    "        # self.upsample_1 = torch.nn.Upsample(scale_factor=2)\n",
    "        self.deconv_3 = torch.nn.ConvTranspose2d(dims, in_channels, 2, stride=2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \"\"\"\n",
    "\n",
    "        # x_ = self.upsample_3(x_)\n",
    "        # print(f\"Output of upsample 3: {x_.shape}\")\n",
    "        x_ = self.deconv_1(x)\n",
    "        # print(f\"Output of deconv 1: {x_.shape}\")\n",
    "  \n",
    "        # x_ = self.upsample_2(x_)\n",
    "        # print(f\"Output of upsample 2: {x_.shape}\")\n",
    "        x_ = self.deconv_2(x_)\n",
    "        # print(f\"Output of  deconv 2: {x_.shape}\")\n",
    "\n",
    "        # x_ = self.upsample_1(x_)\n",
    "        # print(f\"Output of upsample 1: {x_.shape}\")\n",
    "        x_ = self.deconv_3(x_)\n",
    "        # print(f\"Output of deconv 3: {x_.shape}\")\n",
    "\n",
    "        return x_\n",
    "\n",
    "class ShapeModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int, dims: int, num_factors: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = ShapeModelEncoder(in_channels, dims)\n",
    "\n",
    "        self.mean = torch.nn.Linear(2*dims*8*8, num_factors)\n",
    "        self.log_var = torch.nn.Linear(2*dims*8*8, num_factors)\n",
    "\n",
    "        self.rng_state = torch.Generator()\n",
    "\n",
    "        self.upsampler = torch.nn.Upsample(scale_factor=8)\n",
    "\n",
    "        self.decoder = ShapeModelDecoder(num_factors, dims, in_channels)\n",
    "\n",
    "    def reparameterize(self, mean: torch.Tensor, log_var: torch.Tensor):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        std_val = torch.exp(log_var/2)\n",
    "\n",
    "        # Sample noise from gaussian\n",
    "        cov_matrix = torch.eye(mean.shape[-1])\n",
    "        noise = torch.normal(0.0, cov_matrix)\n",
    "\n",
    "        return mean + torch.matmul(std_val, noise)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \"\"\"\n",
    "\n",
    "        # print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "        encoder_output = F.relu(self.encoder(x))\n",
    "        # print(f\"Encoder output shape: {encoder_output.shape}\")\n",
    "\n",
    "        flattened_output = torch.flatten(encoder_output, start_dim=1)\n",
    "        # print(f\"Flattened output: {flattened_output.shape}\")\n",
    "\n",
    "        mean = F.relu(self.mean(flattened_output))\n",
    "        log_var = F.relu(self.log_var(flattened_output))\n",
    "\n",
    "        latent = self.reparameterize(mean, log_var)\n",
    "        # print(f\"Latent shape: {latent.shape}\")\n",
    "\n",
    "        decoder_input = latent.unsqueeze(-1).unsqueeze(-1)\n",
    "        # print(f\"Decoder input: {decoder_input.shape}\")\n",
    "\n",
    "        decoder_input = self.upsampler(decoder_input)\n",
    "        # print(f\"Decoder input: {decoder_input.shape}\")\n",
    "\n",
    "        decoder_output = F.relu(self.decoder(decoder_input))\n",
    "        # print(f\"Decoder output shape: {decoder_output.shape}\")\n",
    "\n",
    "        return decoder_output, mean, log_var\n",
    "\n",
    "    # def sample(self):\n",
    "    #     \"\"\"\n",
    "    #     Sample an image\n",
    "    #     \"\"\"\n",
    "\n",
    "    #     decoder_output = self.decoder(encoder_output)\n",
    "    #     return decoder_output\n",
    "\n",
    "def create_model(in_dimensions: int, dims: int, num_factors: int):\n",
    "    \"\"\"\n",
    "    Create model\n",
    "    \"\"\"\n",
    "\n",
    "    model = ShapeModel(in_dimensions, dims, num_factors)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(input: torch.Tensor, output: torch.Tensor, mean: torch.Tensor, log_var: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compute VAE Loss\n",
    "    \"\"\"\n",
    "\n",
    "    # Sum over each subset & average over each batch\n",
    "    mse_loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "    mse_loss = mse_loss_fn(output, input)\n",
    "    kl_loss = - 0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "\n",
    "    loss = mse_loss + 0.002 * kl_loss\n",
    "\n",
    "    return loss, mse_loss, kl_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_loop(epoch: int, model, dataloader,\n",
    "              wandb_run, accelerator: Accelerator):\n",
    "    \"\"\"\n",
    "    Evaluation loop\n",
    "    \"\"\"\n",
    "\n",
    "    dataframe = []\n",
    "\n",
    "    avg_total_loss = 0\n",
    "    avg_mse_loss = 0\n",
    "    avg_kl_loss = 0\n",
    "\n",
    "    for _, batch in enumerate(dataloader):\n",
    "\n",
    "        print(batch['image'])\n",
    "        raise\n",
    "        pred_image, mean, log_var = model(batch['image'])\n",
    "        # labels = batch['labels']\n",
    "        loss, mse_loss, kl_loss = compute_loss(batch['image'], pred_image, mean, log_var)\n",
    "\n",
    "        avg_total_loss += loss.item()\n",
    "        avg_mse_loss += mse_loss.item()\n",
    "        avg_kl_loss += kl_loss.item()\n",
    "\n",
    "        # TODO: Add FID Score\n",
    "\n",
    "        images = []\n",
    "        pred_images = []\n",
    "\n",
    "        for j in range(batch['image'].shape[0]):\n",
    "            images.append(batch['image'][j, :])\n",
    "            pred_images.append(pred_image[j, :])\n",
    "    \n",
    "        batch_dataframe = pd.DataFrame()\n",
    "        batch_dataframe['epoch'] = [epoch for _ in range(len(images))]\n",
    "        batch_dataframe['image'] = \\\n",
    "            [wandb.Image(image) for image in images]\n",
    "        batch_dataframe['pred_image'] = \\\n",
    "            [wandb.Image(image) for image in pred_images]\n",
    "\n",
    "        dataframe.append(batch_dataframe)\n",
    "\n",
    "    dataframe = pd.concat(dataframe, axis=0, ignore_index=True)\n",
    "\n",
    "    avg_total_loss = avg_total_loss/len(dataloader)\n",
    "    avg_mse_loss = avg_mse_loss/len(dataloader)\n",
    "    avg_kl_loss = avg_kl_loss/len(dataloader)\n",
    "\n",
    "    metrics_str = f\"Val total loss: {avg_total_loss} - MSE loss: {avg_mse_loss} - KL loss: {avg_kl_loss}\"\n",
    "    if accelerator:\n",
    "        accelerator.print(metrics_str)\n",
    "    else:\n",
    "        print(metrics_str)\n",
    "\n",
    "    table = wandb.Table(data=dataframe)\n",
    "    # wandb_run.log({'accuracy': acc}, commit=False)\n",
    "    if wandb_run:\n",
    "        wandb_run.log({'val-total-loss': avg_total_loss}, commit=False)\n",
    "        wandb_run.log({'val-mse-loss': avg_mse_loss}, commit=False)\n",
    "        wandb_run.log({'val-kl-loss': avg_kl_loss}, commit=False)\n",
    "\n",
    "        wandb_run.log({'eval-table': table})\n",
    "\n",
    "def training_loop(config: Namespace, debug_mode=False):\n",
    "    \"\"\"\n",
    "    Training loop\n",
    "    \"\"\"\n",
    "\n",
    "    wandb_run = None\n",
    "    if not debug_mode:\n",
    "        wandb_run = wandb.init(project=config.project_name, entity=None,\n",
    "                            job_type='training',\n",
    "                            name=config.run_name,\n",
    "                            config=config)\n",
    "\n",
    "    accelerator = None\n",
    "    if not debug_mode:\n",
    "        set_seed(config.seed)\n",
    "\n",
    "        grad_accumulation_plugin = GradientAccumulationPlugin(\n",
    "            num_steps=config.grad_accumulation_steps,\n",
    "            adjust_scheduler=True,\n",
    "            sync_with_dataloader=True)\n",
    "\n",
    "        accelerator = Accelerator(\n",
    "            mixed_precision=config.mixed_precision,\n",
    "            gradient_accumulation_plugin=grad_accumulation_plugin,\n",
    "            cpu=(config.device == 'cpu'))\n",
    "\n",
    "    train_dataloader, val_dataloader, dataset = prepare_dataloader(config)\n",
    "    model = create_model(dataset.image_shape[-1], config.hidden_dims, dataset.num_labels)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "#     scheduler = CosineAnnealingLR(\n",
    "#         optimizer,\n",
    "#         T_max=config.num_train_epochs)\n",
    "    scheduler = ExponentialLR(\n",
    "        optimizer,\n",
    "        config.lr_exp_schedule_gamma)\n",
    "\n",
    "#     scheduler = CosineAnnealingWarmRestarts(\n",
    "#         optimizer,\n",
    "#         T_0=config.lr_warmup_steps)\n",
    "        # last_epoch=config.num_train_epochs*len(train_dataloader))\n",
    "\n",
    "    if accelerator:\n",
    "        model, optimizer, train_dataloader, val_dataloader, scheduler = accelerator.prepare(\n",
    "            model, optimizer, train_dataloader, val_dataloader, scheduler)\n",
    "\n",
    "    num_steps = 0\n",
    "    for epoch in range(config.num_train_epochs):\n",
    "        model.train()\n",
    "\n",
    "        epoch_str = f\"---------------- Epoch {epoch} ----------------\"\n",
    "        if accelerator:\n",
    "            accelerator.print(epoch_str)\n",
    "        else:\n",
    "            print(epoch_str)\n",
    "\n",
    "        epoch_total_loss = 0\n",
    "        epoch_mse_loss = 0\n",
    "        epoch_kl_loss = 0\n",
    "\n",
    "        num_iters = 0\n",
    "\n",
    "        for _, batch in enumerate(train_dataloader):\n",
    "            # with accelerator.accumulate(model):\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            pred_image, mean, log_var = model(batch['image'])\n",
    "            # labels = batch['labels']\n",
    "            # print(mean, log_var)\n",
    "\n",
    "            loss, mse_loss, kl_loss = compute_loss(batch['image'], pred_image, mean, log_var)\n",
    "\n",
    "            # accelerator.print(f\"Loss: {loss.item()}\")\n",
    "            if accelerator:\n",
    "                accelerator.backward(loss)\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            epoch_total_loss += loss.item()\n",
    "            epoch_mse_loss += mse_loss.item()\n",
    "            epoch_kl_loss += kl_loss.item()\n",
    "\n",
    "            if wandb_run:\n",
    "                wandb_run.log({'loss': loss.item()}, commit=False, step=num_steps)\n",
    "                wandb_run.log({'mse-loss': mse_loss.item()}, commit=False, step=num_steps)\n",
    "                wandb_run.log({'kl-loss': kl_loss.item()}, commit=False, step=num_steps)\n",
    "\n",
    "                wandb_run.log({'lr': scheduler.get_lr()[0]}, commit=False, step=num_steps)\n",
    "            else:\n",
    "                print(f\"Batch: {loss.item()}, {mse_loss.item()}, {kl_loss.item()}, {scheduler.get_lr()[0]}\")\n",
    "\n",
    "            num_steps += 1\n",
    "            num_iters += 1\n",
    "\n",
    "            # Update the model parameters with the optimizer\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validate model\n",
    "        eval_model_str = \"Evaluating model\"\n",
    "        if accelerator:\n",
    "            accelerator.print(eval_model_str)\n",
    "        else:\n",
    "            print(eval_model_str)\n",
    "\n",
    "        eval_loop(epoch, model, val_dataloader, wandb_run, accelerator)\n",
    "\n",
    "        if wandb_run:\n",
    "            wandb_run.log({'epoch-total-loss': epoch_total_loss/num_iters})\n",
    "            wandb_run.log({'epoch-mse-loss': epoch_mse_loss/num_iters})\n",
    "            wandb_run.log({'epoch-kl-loss': epoch_kl_loss/num_iters})\n",
    "        else:\n",
    "            print(f\"Epoch: {epoch_total_loss}, {epoch_mse_loss}, {epoch_kl_loss}\")\n",
    "\n",
    "    if config.save_model:\n",
    "        # Save model to W&Bs\n",
    "        model_art = wandb.Artifact(config.model_name, type='model')\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "\n",
    "        model_art.add_file('model.pt')\n",
    "        wandb_run.log_artifact(model_art)\n",
    "    wandb_run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- Epoch 0 ----------------\n",
      "Batch: 0.6280412077903748, 0.6276530623435974, 0.194059818983078, 0.0004\n",
      "Batch: 0.6822159290313721, 0.6822144985198975, 0.000708162784576416, 0.0004\n",
      "Batch: 0.6380550861358643, 0.6380550861358643, -0.0, 0.0004\n",
      "Batch: 0.6321941614151001, 0.6321941614151001, -0.0, 0.0004\n",
      "Batch: 0.6048686504364014, 0.6048686504364014, -0.0, 0.0004\n",
      "Batch: 0.6682799458503723, 0.6682799458503723, -0.0, 0.0004\n",
      "Batch: 0.6395112872123718, 0.6395108103752136, 0.0002484321594238281, 0.0004\n",
      "Batch: 0.6079604029655457, 0.6079604029655457, -0.0, 0.0004\n",
      "Batch: 0.6508994698524475, 0.6508994698524475, -0.0, 0.0004\n",
      "Batch: 0.6501573920249939, 0.6501573920249939, -0.0, 0.0004\n",
      "Batch: 0.6152146458625793, 0.6152146458625793, -0.0, 0.0004\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[65], line 142\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(config, debug_mode)\u001b[0m\n\u001b[1;32m    138\u001b[0m epoch_kl_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    140\u001b[0m num_iters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 142\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# with accelerator.accumulate(model):\u001b[39;49;00m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/disentangled-representation-learning-46QTZh80-py3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/disentangled-representation-learning-46QTZh80-py3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/disentangled-representation-learning-46QTZh80-py3.11/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/disentangled-representation-learning-46QTZh80-py3.11/lib/python3.11/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/disentangled-representation-learning-46QTZh80-py3.11/lib/python3.11/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[62], line 15\u001b[0m, in \u001b[0;36mShapeDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m---> 15\u001b[0m     img_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mh5_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     16\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh5_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m][index]\n\u001b[1;32m     18\u001b[0m     img_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(img_array, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/disentangled-representation-learning-46QTZh80-py3.11/lib/python3.11/site-packages/h5py/_hl/dataset.py:781\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, args, new_dtype)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fast_read_ok \u001b[38;5;129;01mand\u001b[39;00m (new_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 781\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fast_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall back to Python read pathway below\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_loop(CONFIG, debug_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from accelerate import notebook_launcher\n",
    "\n",
    "# notebook_launcher(training_loop, (CONFIG, ), num_processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pil_to_tensor = torchvision.transforms.PILToTensor()\n",
    "# # tensor_to_pil = torchvision.transforms.ToPILImage()\n",
    "# img_tensor = pil_to_tensor(img).type(torch.float32)\n",
    "# img_tensor = img_tensor.unsqueeze(0)\n",
    "# output_img = None\n",
    "# with torch.no_grad():\n",
    "#     output = model(img_tensor)\n",
    "#     output_img = output.squeeze(0).transpose(0, 2)\n",
    "# tensor_to_pil(output_img.type(torch.uint8).numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disentangled-representation-learning-46QTZh80-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
