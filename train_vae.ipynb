{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from diffusers import get_cosine_schedule_with_warmup\n",
    "\n",
    "import wandb\n",
    "\n",
    "import pandas as pd\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate.utils import GradientAccumulationPlugin\n",
    "from accelerate.utils import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate.utils import write_basic_config\n",
    "\n",
    "write_basic_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE = torch.device(\n",
    "#     'cuda' if torch.cuda.is_available() \\\n",
    "#         else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "HIDDEN_DIMS = 64\n",
    "\n",
    "CONFIG = Namespace(\n",
    "    project_name=\"3dshapes\",\n",
    "    run_name=f'3dshapes-vae-{HIDDEN_DIMS}-1',\n",
    "    model_name=f'3dshapes-vae-{HIDDEN_DIMS}-model-v1',\n",
    "    hidden_dims=HIDDEN_DIMS,\n",
    "    dataset_path='3dshapes.h5',\n",
    "    # horizontal_flip_prob=0.5,\n",
    "    # gaussian_blur_kernel_size=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    val_steps=500,\n",
    "    learning_rate=4e-4,\n",
    "    seed=1,\n",
    "    beta_schedule='squaredcos_cap_v2',\n",
    "    lr_exp_schedule_gamma=0.85,\n",
    "    lr_warmup_steps=100,\n",
    "    train_limit=-1,\n",
    "    save_model=True,\n",
    "    mixed_precision=None,\n",
    "    grad_accumulation_steps=4\n",
    "    )\n",
    "CONFIG.device = DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_TRANSFORMS = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),  # Convert to tensor (0, 1)\n",
    "        transforms.Normalize([0.5], [0.5]),  # Map to (-1, 1)\n",
    "    ])\n",
    "class ShapeDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, h5_data: h5py.File) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.h5_data = h5_data\n",
    "        self.image_shape = self.h5_data['images'][0].shape\n",
    "        self.num_labels = self.h5_data['labels'][0].shape[0]\n",
    "        self.normalize_transform = transforms.Normalize([0.5], [0.5]) # Map to (-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.h5_data['images'])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_array = self.h5_data['images'][index]\n",
    "        labels = self.h5_data['labels'][index]\n",
    "\n",
    "        # Image tensor will be 64x64x3\n",
    "        img_tensor = IMG_TRANSFORMS(img_array)\n",
    "        # img_tensor = torch.tensor(img_array, dtype=torch.float32)\n",
    "        # img_tensor = img_tensor.transpose(0, 2)\n",
    "\n",
    "        # img_tensor = img_tensor/255.0\n",
    "        # print(f\"Image tensor before: {img_tensor.shape}\")\n",
    "        img_tensor = self.normalize_transform(img_tensor)\n",
    "        # print(f\"Image tensor after: {img_tensor}\")\n",
    "        labels_tensor = torch.tensor(labels)\n",
    "\n",
    "        # print(f\"Image tensor shape: {img_tensor.shape}\")\n",
    "        # print(f\"Labels tensor shape: {labels_tensor.shape}\")\n",
    "\n",
    "        output = {\n",
    "            'image': img_tensor,\n",
    "            'labels': labels_tensor\n",
    "            }\n",
    "\n",
    "        return output\n",
    "    \n",
    "def create_dataset(config: Namespace):\n",
    "    \"\"\"\n",
    "    Create dataset\n",
    "    \"\"\"\n",
    "\n",
    "    data = h5py.File(config.dataset_path, 'r')\n",
    "    shape_dataset = ShapeDataset(data)\n",
    "    return shape_dataset\n",
    "\n",
    "def prepare_dataloader(config: Namespace):\n",
    "    \"\"\"\n",
    "    Prepare dataloader\n",
    "    \"\"\"\n",
    "\n",
    "    shape_dataset = create_dataset(config)\n",
    "\n",
    "    generator = torch.Generator().manual_seed(config.seed)\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(shape_dataset, [0.80, 0.20], generator)\n",
    "\n",
    "    train_gen = torch.Generator().manual_seed(config.seed)\n",
    "    val_gen = torch.Generator().manual_seed(config.seed)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=config.per_device_train_batch_size,\n",
    "        shuffle=True, generator=train_gen)\n",
    "\n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=config.per_device_eval_batch_size,\n",
    "        shuffle=False, generator=val_gen)\n",
    "\n",
    "    return train_dataloader, val_dataloader, shape_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ShapeModelEncoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int, dims: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_1 = torch.nn.Conv2d(\n",
    "            in_channels, dims, kernel_size=3, padding='same')\n",
    "        self.max_pool_1 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv_2 = torch.nn.Conv2d(\n",
    "            dims, 2*dims, kernel_size=3, padding='same')\n",
    "        self.max_pool_2 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.conv_3 = torch.nn.Conv2d(\n",
    "            2*dims, 2*dims, kernel_size=3, padding='same')\n",
    "        self.max_pool_3 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \"\"\"\n",
    "\n",
    "        x_ = self.conv_1(x)\n",
    "        # print(f\"Output of conv 1: {x_.shape}\")\n",
    "        x_ = F.relu(self.max_pool_1(x_))\n",
    "        # print(f\"Output of conv & max pool 1: {x_.shape}\")\n",
    "\n",
    "        x_ = self.conv_2(x_)\n",
    "        # print(f\"Output of conv 2: {x_.shape}\")\n",
    "        x_ = F.relu(self.max_pool_2(x_))\n",
    "        # print(f\"Output of conv & max pool 2: {x_.shape}\")\n",
    "\n",
    "        x_ = self.conv_3(x_)\n",
    "        # print(f\"Output of conv 3: {x_.shape}\")\n",
    "        x_ = F.relu(self.max_pool_3(x_))\n",
    "        # print(f\"Output of conv & max pool 3: {x_.shape}\")\n",
    "\n",
    "        return x_\n",
    "\n",
    "class ShapeModelDecoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dims: int, in_channels: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # NOTE: I still don't understand how transpose convolution works\n",
    "        # self.upsample_3 = torch.nn.Upsample(scale_factor=2)\n",
    "        self.deconv_1 = torch.nn.ConvTranspose2d(2*dims, 2*dims, 2, stride=2)\n",
    "\n",
    "        # self.deconv_2 = torch.nn.ConvTranspose2d(2*dims, 2*dims, 2, stride=2)\n",
    "\n",
    "        # self.deconv_3 = torch.nn.ConvTranspose2d(2*dims, 2*dims, 2, stride=2)\n",
    "\n",
    "        # self.deconv_4 = torch.nn.ConvTranspose2d(2*dims, dims, 2, stride=2)\n",
    "\n",
    "        # self.upsample_2 = torch.nn.Upsample(scale_factor=2)\n",
    "        self.deconv_2 = torch.nn.ConvTranspose2d(2*dims, dims, 2, stride=2)\n",
    "\n",
    "        # self.upsample_1 = torch.nn.Upsample(scale_factor=2)\n",
    "        self.deconv_3 = torch.nn.ConvTranspose2d(dims, in_channels, 2, stride=2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \"\"\"\n",
    "\n",
    "        # x_ = self.upsample_3(x_)\n",
    "        # print(f\"Output of upsample 3: {x_.shape}\")\n",
    "        x_ = F.relu(self.deconv_1(x))\n",
    "        # print(f\"Output of deconv 1: {x_.shape}\")\n",
    "  \n",
    "        # x_ = self.upsample_2(x_)\n",
    "        # print(f\"Output of upsample 2: {x_.shape}\")\n",
    "        x_ = F.relu(self.deconv_2(x_))\n",
    "        # print(f\"Output of  deconv 2: {x_.shape}\")\n",
    "\n",
    "        # x_ = self.upsample_1(x_)\n",
    "        # print(f\"Output of upsample 1: {x_.shape}\")\n",
    "\n",
    "        # We want the decoder output to be between -1 and 1\n",
    "        x_ = F.tanh(self.deconv_3(x_))\n",
    "        # print(f\"Output of deconv 3: {x_.shape}\")\n",
    "\n",
    "        # x_ = F.relu(self.deconv_4(x_))\n",
    "        # # print(f\"Output of deconv 4: {x_.shape}\")\n",
    "        # x_ = F.relu(self.deconv_5(x_))\n",
    "        # # print(f\"Output of deconv 5: {x_.shape}\")\n",
    "        # x_ = F.relu(self.deconv_6(x_))\n",
    "        # # print(f\"Output of deconv 6: {x_.shape}\")\n",
    "\n",
    "        return x_\n",
    "\n",
    "class ShapeModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int, dims: int, num_factors: int, seed: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dims = dims\n",
    "\n",
    "        self.encoder = ShapeModelEncoder(in_channels, dims)\n",
    "\n",
    "        self.mean = torch.nn.Linear(2*dims*8*8, num_factors)\n",
    "        self.log_var = torch.nn.Linear(2*dims*8*8, num_factors)\n",
    "\n",
    "        self.rng_state = torch.Generator().manual_seed(seed)\n",
    "\n",
    "        self.latent_decoder = torch.nn.Linear(num_factors, 2*dims*8*8)\n",
    "        self.decoder = ShapeModelDecoder(dims, in_channels)\n",
    "\n",
    "    def reparameterize(self, mean: torch.Tensor, log_var: torch.Tensor):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        std_val = torch.exp(log_var/2)\n",
    "\n",
    "        # Sample noise from gaussian\n",
    "        cov_matrix = torch.eye(mean.shape[-1])\n",
    "        noise = torch.normal(0.0, cov_matrix)\n",
    "\n",
    "        return mean + torch.matmul(std_val, noise)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \"\"\"\n",
    "\n",
    "        # print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "        encoder_output = self.encoder(x)\n",
    "        # print(f\"Encoder output shape: {encoder_output.shape}\")\n",
    "\n",
    "        flattened_output = torch.flatten(encoder_output, start_dim=1)\n",
    "        # print(f\"Flattened output: {flattened_output.shape}\")\n",
    "\n",
    "        mean = self.mean(flattened_output)\n",
    "        log_var = self.log_var(flattened_output)\n",
    "\n",
    "        latent = self.reparameterize(mean, log_var)\n",
    "        # print(f\"Latent shape: {latent.shape}\")\n",
    "\n",
    "        decoder_input = F.relu(self.latent_decoder(latent))\n",
    "        # print(f\"Decoder input (before reshape): {decoder_input.shape}\")\n",
    "\n",
    "        decoder_input = decoder_input.reshape(\n",
    "            (decoder_input.shape[0], 2*self.hidden_dims, 8, 8))\n",
    "        # decoder_input = latent.unsqueeze(-1).unsqueeze(-1)\n",
    "        # print(f\"Decoder input: {decoder_input.shape}\")\n",
    "\n",
    "        # decoder_input = self.upsampler(decoder_input)\n",
    "        # print(f\"Decoder input: {decoder_input.shape}\")\n",
    "\n",
    "        decoder_output = self.decoder(decoder_input)\n",
    "\n",
    "        # print(f\"Decoder output shape: {decoder_output.shape}\")\n",
    "\n",
    "        return decoder_output, mean, log_var\n",
    "\n",
    "def create_model(in_dimensions: int, dims: int, num_factors: int, seed: int):\n",
    "    \"\"\"\n",
    "    Create model\n",
    "    \"\"\"\n",
    "\n",
    "    model = ShapeModel(in_dimensions, dims, num_factors, seed)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(input: torch.Tensor, output: torch.Tensor, mean: torch.Tensor, log_var: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Compute VAE Loss\n",
    "    \"\"\"\n",
    "\n",
    "    # Sum over each subset & average over each batch\n",
    "    mse_loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "    mse_loss = mse_loss_fn(output, input)\n",
    "    kl_loss = - 0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "\n",
    "    loss = mse_loss + kl_loss\n",
    "\n",
    "    return loss, mse_loss, kl_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_loop(epoch: int, model, dataloader,\n",
    "              wandb_run, accelerator: Accelerator):\n",
    "    \"\"\"\n",
    "    Evaluation loop\n",
    "    \"\"\"\n",
    "\n",
    "    dataframe = []\n",
    "\n",
    "    avg_total_loss = 0\n",
    "    avg_mse_loss = 0\n",
    "    avg_kl_loss = 0\n",
    "\n",
    "    tensor_to_pil_fn = transforms.ToPILImage()\n",
    "\n",
    "    for _, batch in enumerate(dataloader):\n",
    "\n",
    "        pred_image, mean, log_var = model(batch['image'])\n",
    "        # labels = batch['labels']\n",
    "        loss, mse_loss, kl_loss = compute_loss(batch['image'], pred_image, mean, log_var)\n",
    "\n",
    "        avg_total_loss += loss.item()\n",
    "        avg_mse_loss += mse_loss.item()\n",
    "        avg_kl_loss += kl_loss.item()\n",
    "\n",
    "        # TODO: Add FID Score\n",
    "\n",
    "        images = []\n",
    "        pred_images = []\n",
    "\n",
    "        for j in range(batch['image'].shape[0]):\n",
    "            _img = tensor_to_pil_fn((batch['image'][j, :]*0.5)+0.5)\n",
    "            _pred_img  = tensor_to_pil_fn((pred_image[j, :]*0.5)+0.5)\n",
    "\n",
    "            images.append(_img)\n",
    "            pred_images.append(_pred_img)\n",
    "    \n",
    "        batch_dataframe = pd.DataFrame()\n",
    "        batch_dataframe['epoch'] = [epoch for _ in range(len(images))]\n",
    "        batch_dataframe['image'] = \\\n",
    "            [wandb.Image(image) for image in images]\n",
    "        batch_dataframe['pred_image'] = \\\n",
    "            [wandb.Image(image) for image in pred_images]\n",
    "\n",
    "        dataframe.append(batch_dataframe)\n",
    "\n",
    "    dataframe = pd.concat(dataframe, axis=0, ignore_index=True)\n",
    "\n",
    "    avg_total_loss = avg_total_loss/len(dataloader)\n",
    "    avg_mse_loss = avg_mse_loss/len(dataloader)\n",
    "    avg_kl_loss = avg_kl_loss/len(dataloader)\n",
    "\n",
    "    metrics_str = f\"Val total loss: {avg_total_loss} - MSE loss: {avg_mse_loss} - KL loss: {avg_kl_loss}\"\n",
    "    if accelerator:\n",
    "        accelerator.print(metrics_str)\n",
    "    else:\n",
    "        print(metrics_str)\n",
    "\n",
    "    table = wandb.Table(data=dataframe)\n",
    "    # wandb_run.log({'accuracy': acc}, commit=False)\n",
    "    if wandb_run:\n",
    "        wandb_run.log({'epoch': epoch, 'val_total_loss': avg_total_loss}, commit=False)\n",
    "        wandb_run.log({'epoch': epoch, 'val_mse_loss': avg_mse_loss}, commit=False)\n",
    "        wandb_run.log({'epoch': epoch, 'val_kl_loss': avg_kl_loss}, commit=False)\n",
    "\n",
    "        wandb_run.log({'epoch': epoch, 'eval_table': table}, commit=False)\n",
    "\n",
    "def training_loop(config: Namespace, debug_mode=False):\n",
    "    \"\"\"\n",
    "    Training loop\n",
    "    \"\"\"\n",
    "\n",
    "    wandb_run = None\n",
    "    if not debug_mode:\n",
    "        wandb_run = wandb.init(project=config.project_name, entity=None,\n",
    "                            job_type='training',\n",
    "                            name=config.run_name,\n",
    "                            config=config)\n",
    "\n",
    "        wandb_run.define_metric(\"epoch\")  \n",
    "        wandb_run.define_metric(\"training_step\")  \n",
    "\n",
    "        wandb_run.define_metric('val_total_loss', step_metric='epoch')\n",
    "        wandb_run.define_metric('val_mse_loss', step_metric='epoch')\n",
    "        wandb_run.define_metric('val_kl_loss', step_metric='epoch')\n",
    "        wandb_run.define_metric('eval_table', step_metric='epoch')\n",
    "\n",
    "        wandb_run.define_metric('step_loss', step_metric='training_step')\n",
    "        wandb_run.define_metric('step_mse_loss', step_metric='training_step')\n",
    "        wandb_run.define_metric('step_kl_loss', step_metric='training_step')\n",
    "\n",
    "        wandb_run.define_metric('epoch_loss', step_metric='training_step')\n",
    "        wandb_run.define_metric('epoch_mse_loss', step_metric='training_step')\n",
    "        wandb_run.define_metric('epoch_kl_loss', step_metric='training_step')\n",
    "\n",
    "        wandb_run.define_metric('lr', step_metric='training_step')\n",
    "\n",
    "    accelerator = None\n",
    "    if not debug_mode:\n",
    "        set_seed(config.seed)\n",
    "\n",
    "        grad_accumulation_plugin = GradientAccumulationPlugin(\n",
    "            num_steps=config.grad_accumulation_steps,\n",
    "            adjust_scheduler=True,\n",
    "            sync_with_dataloader=True)\n",
    "\n",
    "        accelerator = Accelerator(\n",
    "            mixed_precision=config.mixed_precision,\n",
    "            gradient_accumulation_plugin=grad_accumulation_plugin,\n",
    "            cpu=(config.device == 'cpu'))\n",
    "\n",
    "    train_dataloader, val_dataloader, dataset = prepare_dataloader(config)\n",
    "    model = create_model(dataset.image_shape[-1], config.hidden_dims, dataset.num_labels, config.seed)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "#     scheduler = CosineAnnealingLR(\n",
    "#         optimizer,\n",
    "#         T_max=config.num_train_epochs)\n",
    "    # scheduler = ExponentialLR(\n",
    "    #     optimizer,\n",
    "    #     config.lr_exp_schedule_gamma)\n",
    "\n",
    "#     scheduler = CosineAnnealingWarmRestarts(\n",
    "#         optimizer,\n",
    "#         T_0=config.lr_warmup_steps)\n",
    "        # last_epoch=config.num_train_epochs*len(train_dataloader))\n",
    "\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer, config.lr_warmup_steps, len(train_dataloader)*config.num_train_epochs)\n",
    "\n",
    "    if accelerator:\n",
    "        model, optimizer, train_dataloader, val_dataloader, scheduler = accelerator.prepare(\n",
    "            model, optimizer, train_dataloader, val_dataloader, scheduler)\n",
    "\n",
    "    num_steps = 0\n",
    "    for epoch in range(config.num_train_epochs):\n",
    "        model.train()\n",
    "\n",
    "        epoch_str = f\"---------------- Epoch {epoch} ----------------\"\n",
    "        if accelerator:\n",
    "            accelerator.print(epoch_str)\n",
    "        else:\n",
    "            print(epoch_str)\n",
    "\n",
    "        epoch_total_loss = 0\n",
    "        epoch_mse_loss = 0\n",
    "        epoch_kl_loss = 0\n",
    "\n",
    "        num_iters = 0\n",
    "\n",
    "        for _, batch in enumerate(train_dataloader):\n",
    "            # with accelerator.accumulate(model):\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            pred_image, mean, log_var = model(batch['image'])\n",
    "            # labels = batch['labels']\n",
    "            # print(mean, log_var)\n",
    "\n",
    "            loss, mse_loss, kl_loss = compute_loss(batch['image'], pred_image, mean, log_var)\n",
    "\n",
    "            # accelerator.print(f\"Loss: {loss.item()}\")\n",
    "            if accelerator:\n",
    "                accelerator.backward(loss)\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            epoch_total_loss += loss.item()\n",
    "            epoch_mse_loss += mse_loss.item()\n",
    "            epoch_kl_loss += kl_loss.item()\n",
    "\n",
    "            if wandb_run:\n",
    "                wandb_run.log({'training_step': num_steps, 'step_loss': loss.item()})\n",
    "                wandb_run.log({'training_step': num_steps, 'step_mse_loss': mse_loss.item()})\n",
    "                wandb_run.log({'training_step': num_steps, 'step_kl_loss': kl_loss.item()})\n",
    "                wandb_run.log({'training_step': num_steps, 'lr': scheduler.get_lr()[0]})\n",
    "\n",
    "            else:\n",
    "                print(f\"Batch: {loss.item()}, {mse_loss.item()}, {kl_loss.item()}, {scheduler.get_lr()[0]}\")\n",
    "\n",
    "            num_steps += 1\n",
    "            num_iters += 1\n",
    "\n",
    "            # Update the model parameters with the optimizer\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Validate model\n",
    "            if num_steps % config.val_steps == 0:\n",
    "                eval_model_str = \"Evaluating model\"\n",
    "                if accelerator:\n",
    "                    accelerator.print(eval_model_str)\n",
    "                else:\n",
    "                    print(eval_model_str)\n",
    "\n",
    "                eval_loop(num_steps, model, val_dataloader, wandb_run, accelerator)\n",
    "\n",
    "        if wandb_run:\n",
    "            wandb_run.log({'training_step': num_steps, 'epoch_loss': epoch_total_loss/num_iters})\n",
    "            wandb_run.log({'training_step': num_steps, 'epoch_mse_loss': epoch_mse_loss/num_iters})\n",
    "            wandb_run.log({'training_step': num_steps, 'epoch_kl_loss': epoch_kl_loss/num_iters})\n",
    "        else:\n",
    "            print(f\"Epoch: {epoch_total_loss}, {epoch_mse_loss}, {epoch_kl_loss}\")\n",
    "\n",
    "    if config.save_model:\n",
    "        # Save model to W&Bs\n",
    "        model_art = wandb.Artifact(config.model_name, type='model')\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "\n",
    "        model_art.add_file('model.pt')\n",
    "        wandb_run.log_artifact(model_art)\n",
    "    wandb_run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Training + Validation code\n",
    "# training_loop(CONFIG, debug_mode=True)\n",
    "\n",
    "# Testing Validation code\n",
    "# train_dataloader, val_dataloader, dataset = prepare_dataloader(CONFIG)\n",
    "# model = create_model(dataset.image_shape[-1], CONFIG.hidden_dims, dataset.num_labels, CONFIG.seed)\n",
    "\n",
    "# eval_loop(0, model, val_dataloader, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_loop, (CONFIG, ), num_processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pil_to_tensor = torchvision.transforms.PILToTensor()\n",
    "# # tensor_to_pil = torchvision.transforms.ToPILImage()\n",
    "# img_tensor = pil_to_tensor(img).type(torch.float32)\n",
    "# img_tensor = img_tensor.unsqueeze(0)\n",
    "# output_img = None\n",
    "# with torch.no_grad():\n",
    "#     output = model(img_tensor)\n",
    "#     output_img = output.squeeze(0).transpose(0, 2)\n",
    "# tensor_to_pil(output_img.type(torch.uint8).numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae-playground-wPvColg4-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
